{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Verification\n",
    "Verifying whether a pair of face images belong to the same person irrespective of lighting conditions, age and other nuisance factors.\n",
    "\n",
    "## Motivation\n",
    "- Surveillance: Identify a person in one of the footages, now can you automatically find the same person in other footages as well? The footages need not be coming from a single camera\n",
    "- Biometry: Give biometric access to a particular area (e.g. hotel room, academic labs etc) by verifying whether the person is eligible to be given access or not\n",
    "\n",
    "## A real story\n",
    "Baidu's face recognition system helps identify the parents of a missing child. Parents upload a picture of his son when he was 4 years old and the missing person uploads his picture when he was 10 years old. Baidu's system recognized the two faces as belonging to the same person and was able to help reunite the son with his parents.\n",
    "\n",
    "https://www.dailymail.co.uk/news/peoplesdaily/article-4492814/Man-33-finds-parents-help-facial-recognition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve the face verification problem\n",
    "\n",
    "Solving face verification robustly requires us to extract features from face images which are invariant to several factors such as age, illumination in the image, face profiles (side/frontal), image resolution etc. However, defining such a feature is a task which seems almost impossible. \n",
    "\n",
    "What if we could let the \"machine learn\" a feature which is invariant to all these factors? However, for this we need a dataset which contains all these different factors. Hence, dataset collection is the most important factor here, as in any other machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "import scipy.misc\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Collect dataset\n",
    "Here, we are using Yale Faces dataset.\n",
    "\n",
    "Characteristics of the Yale Faces dataset\n",
    "- Frontal faces\n",
    "- Faces captured under various lighting conditions\n",
    "- Single face per image with a plain background\n",
    "- Each face has displays various emotions\n",
    "\n",
    "Link to a webpage containing references to other face datasets:\n",
    "\n",
    "http://www.face-rec.org/databases/\n",
    "\n",
    "## Read a few face images from the dataset and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1 = imageio.imread('yalefaces/subject01.glasses')\n",
    "I2 = imageio.imread('yalefaces/subject02.leftlight')\n",
    "I3 = imageio.imread('yalefaces/subject03.sleepy')\n",
    "I4 = imageio.imread('yalefaces/subject02.rightlight')\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(I1,cmap='Greys_r')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(I2,cmap='Greys_r')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(I3,cmap='Greys_r')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(I4,cmap='Greys_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset preprocessing\n",
    "\n",
    "Idea is to keep only that information in the image which helps in face verification, removing all the unnecessary information (usually called nuisance parameters). Lighting condition is one example of nuisance parameter. Can you list two others? The nuisance parameters need not be in this dataset, but you could encounter in a real-world scenario.\n",
    "\n",
    "## Localize faces in the image and crop\n",
    "A technical way of saying - \"get rid of the background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use a builtin opencv function to \n",
    "#localize the faces in an image and draw a bounding box\n",
    "# An example code is shown here on an example image 'abba.png'\n",
    "I = imageio.imread('abba.png')\n",
    "cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    I,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30)\n",
    "#     flags = cv2.CV_HAAR_SCALE_IMAGE\n",
    ")\n",
    "\n",
    "print(\"Found {0} faces!\".format(len(faces)))\n",
    "\n",
    "# Draw a rectangle around the faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(I, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "plt.imshow(I,cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset\n",
    "\n",
    "Read each of the images from the dataset and also crop the face from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the dataset\n",
    "full_face = []\n",
    "full_labels = []\n",
    "all_images = os.listdir('yalefaces')\n",
    "for j in range(1,11):    # there are 15 different people in the dataset\n",
    "    img_list = [filename for filename in all_images if filename.startswith('subject%.2d'%j)]\n",
    "#     img_list = glob.glob('yalefaces/subject%.2d*'%j)\n",
    "    face1 = np.zeros((len(img_list),80,60))\n",
    "    for k,img in enumerate(img_list):\n",
    "        I = imageio.imread(os.path.join('yalefaces',img))\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            I,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30))\n",
    "        x,y,h,w = faces[0]\n",
    "        face1[k,...] = zoom(I[y:y+h,x:x+w],[80./h,60./w])\n",
    "    labels = (j-1)*np.ones((11,))\n",
    "    full_face.append(face1)\n",
    "    full_labels.append(labels)\n",
    "full_face = np.stack(full_face)    # contains the images of all faces\n",
    "full_labels = np.stack(full_labels)   # contains the ID of all the faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualize few faces from the dataset\n",
    "I1 = full_face[0,5,...]\n",
    "I2 = full_face[1,4,...]\n",
    "I3 = full_face[6,3,...]\n",
    "I4 = full_face[5,7,...]\n",
    "plt.subplot(221)\n",
    "plt.imshow(I1,cmap='Greys_r')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(I2,cmap='Greys_r')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(I3,cmap='Greys_r')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(I4,cmap='Greys_r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(img):\n",
    "#     img = img[:]\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "            img,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30))\n",
    "    try:\n",
    "        x,y,h,w = faces[0]\n",
    "        face1 = zoom(img[y:y+h,x:x+w],[80./h,60./w])\n",
    "        if face1.shape[1]!=60:\n",
    "            face1 = resize(face1, (80, 60),anti_aliasing=True,preserve_range= True).astype('uint8')\n",
    "        return face1\n",
    "    except:\n",
    "        img = resize(img, (80, 60),anti_aliasing=True,preserve_range= True).astype('uint8')\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "full_face = full_face.reshape((-1,80,60))\n",
    "full_labels = full_labels.reshape((-1,1)) \n",
    "print(full_face.shape)\n",
    "print(full_labels.shape)\n",
    "\n",
    "hog_features = [hog(n,pixels_per_cell=(5,5)) for n in full_face]\n",
    "hog_features = np.stack(hog_features,0)\n",
    "hog_features = hog_features.reshape(len(full_face),-1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_features, full_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train and test\n",
    "# pick 2 images from each dataset and put it into test set\n",
    "# train_set = np.zeros((full_face.shape[0],9,80,60))\n",
    "# test_set = np.zeros((full_face.shape[0],2,80,60))\n",
    "# train_labels = np.zeros((full_face.shape[0],9))\n",
    "# test_labels = np.zeros((full_face.shape[0],2))\n",
    "# for k in range(full_face.shape[0]):\n",
    "#     g = np.random.permutation(11)   # randomly select the 9 faces\n",
    "#     train_set[k,...] = full_face[k,g[:9],...]\n",
    "#     train_labels[k,...] = full_labels[k,g[:9]]\n",
    "#     test_set = full_face[:,g[9:],...]\n",
    "#     test_labels[k,...] = full_labels[k,g[9:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Feature extraction\n",
    "\n",
    "Note: A possible step before this would be to include additional pre-processing steps which can get rid of some of the nuisance paramters like lighting conditions\n",
    "\n",
    "### Question: Why feature extraction?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### List some features you could extract from the images\n",
    "Keep in mind that these image belong to a specific category which is face and not any arbitrary image\n",
    "\n",
    "#### Face specific features: Need to be learned from a dataset of faces\n",
    "- Eigen faces\n",
    "- Fisher faces\n",
    "#### General features\n",
    "- Histogram of Oriented Gradients (HOG)\n",
    "- Local Binary Pattern Histogram (LBPH)\n",
    "\n",
    "#### References:\n",
    "- Face recognition using Eigenfaces: http://www.cin.ufpe.br/~rps/Artigos/Face%20Recognition%20Using%20Eigenfaces.pdf\n",
    "- Face recognition using Fisherfaces: http://www.dtic.mil/dtic/tr/fulltext/u2/1015508.pdf\n",
    "- LBPH: https://ieeexplore.ieee.org/document/1717463\n",
    "\n",
    "\n",
    "\n",
    "Here, we will explore a non-data driven feature which is local binary pattern\n",
    "\n",
    "![](LBP.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "train_set = np.reshape(train_set,[-1,80,60])\n",
    "train_labels = np.reshape(train_labels,[-1,])\n",
    "test_set = np.reshape(test_set,[-1,80,60])\n",
    "test_labels = np.reshape(test_labels,[-1,])\n",
    "train_feat = np.zeros_like(train_set)\n",
    "test_feat = np.zeros_like(test_set)\n",
    "\n",
    "for k in range(train_feat.shape[0]):\n",
    "        train_feat[k,...] = local_binary_pattern(train_set[k,...],3,3, method='nri_uniform')#,method='uniform')\n",
    "#           fd, hog_image = hog(train_set[k,...], orientations=8, pixels_per_cell=(16, 16),\n",
    "#                     cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "          \n",
    "for k in range(test_feat.shape[0]):\n",
    "        test_feat[k,...] = local_binary_pattern(test_set[k,...],3,3,method='nri_uniform')#,method='uniform')\n",
    "print(np.amax(test_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize an original image and an LBP image\n",
    "idx= 16\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(train_set[idx,...],cmap='Greys_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('LBP Image')\n",
    "plt.imshow(train_feat[idx,...],cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from the LBP image\n",
    "\n",
    "![](LBPH.png '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of grids in x and y direction\n",
    "# divide the image into the specified number of grids\n",
    "# for each grid compute the histogram with 64 bins\n",
    "# concatenate the histogram from all grid regions to form a single feature\n",
    "# now we have one feature vector per image\n",
    "# Note that grid size and histogram bin sizes are variable\n",
    "x_grid = 7\n",
    "y_grid = 7\n",
    "grid_size_x = train_feat.shape[1]//x_grid\n",
    "grid_size_y = train_feat.shape[2]//y_grid\n",
    "for k in range(train_feat.shape[0]):\n",
    "    for x in range(x_grid):\n",
    "        for y in range(y_grid):\n",
    "            patch = train_feat[k,x*grid_size_x:(x+1)*grid_size_x,y*grid_size_y:(y+1)*grid_size_y]\n",
    "            hist,_ = np.histogram(patch.reshape(-1),bins=64)\n",
    "            if x==0 and y==0:\n",
    "                patch_hist_list = hist\n",
    "            else:\n",
    "                patch_hist_list = np.concatenate([patch_hist_list,hist],0)\n",
    "    if k==0:\n",
    "        data_hist = patch_hist_list[np.newaxis,...]\n",
    "    else:\n",
    "        data_hist = np.concatenate([data_hist,patch_hist_list[np.newaxis,...]],0)\n",
    "print(data_hist.shape)\n",
    "\n",
    "grid_size_x = test_feat.shape[1]//x_grid\n",
    "grid_size_y = test_feat.shape[2]//y_grid\n",
    "\n",
    "for k in range(test_feat.shape[0]):\n",
    "    for x in range(x_grid):\n",
    "        for y in range(y_grid):\n",
    "            patch = test_feat[k,x*grid_size_x:(x+1)*grid_size_x,y*grid_size_y:(y+1)*grid_size_y]\n",
    "            hist,_ = np.histogram(patch.reshape(-1),bins=64)\n",
    "            if x==0 and y==0:\n",
    "                patch_hist_list = hist\n",
    "            else:\n",
    "                patch_hist_list = np.concatenate([patch_hist_list,hist],0)\n",
    "    if k==0:\n",
    "        test_data_hist = patch_hist_list[np.newaxis,...]\n",
    "    else:\n",
    "        test_data_hist = np.concatenate([test_data_hist,patch_hist_list[np.newaxis,...]],0)\n",
    "print(test_data_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform dimensionality reduction on the feature vector\n",
    "\n",
    "Number of training samples is way less than the dimensionality of the data, hence reduce the number of data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(whiten=True)\n",
    "train_feat = X_train\n",
    "test_feat = X_test\n",
    "print(train_feat.shape,test_feat.shape)\n",
    "pca.fit(train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of components which explain 99% of the data variance\n",
    "n_components = np.argmin(np.cumsum(pca.explained_variance_ratio_)<=0.99)\n",
    "print(n_components)\n",
    "train_feat_pca = pca.transform(train_feat)\n",
    "train_feat_pca = train_feat_pca[:,:n_components]\n",
    "test_feat_pca = pca.transform(test_feat)\n",
    "test_feat_pca = test_feat_pca[:,:n_components]\n",
    "print(train_feat_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pairs of images with labels as positive pair and negative pairs\n",
    "\n",
    "Positive pairs: Face images of the same person\n",
    "\n",
    "Negative pairs: Face images of different persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "img_pair = []\n",
    "img_full_pair = []\n",
    "label_pair = []\n",
    "for pair in itertools.combinations_with_replacement(range(train_feat_pca.shape[0]),2):\n",
    "    img_pair.append(train_feat_pca[pair,:])\n",
    "    img_full_pair.append(train_set[pair,:])\n",
    "    label_pair.append(y_train[pair,])\n",
    "img_pair = np.stack(img_pair)\n",
    "img_full_pair = np.stack(img_full_pair)\n",
    "label_pair = np.stack(label_pair)\n",
    "label_pair = (label_pair[:,0] == label_pair[:,1])\n",
    "label_pair = label_pair*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a positive pair of faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "img_pair_true = img_full_pair[idx,:]\n",
    "print('positive pair with label: ',label_pair[idx])\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_pair_true[0,...].reshape(80,60),cmap='Greys_r')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_pair_true[1,...].reshape(80,60),cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a negative pair of faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=15\n",
    "img_pair_true = img_full_pair[idx,:]\n",
    "print('negative pair with label: ',label_pair[idx])\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_pair_true[0,...].reshape(80,60),cmap='Greys_r')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_pair_true[1,...].reshape(80,60),cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make pairs out of test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_pair = []\n",
    "test_label_pair = []\n",
    "for pair in itertools.combinations_with_replacement(range(test_feat.shape[0]),2):\n",
    "    test_img_pair.append(test_feat_pca[pair,...])\n",
    "    test_label_pair.append(test_labels[pair,])\n",
    "test_img_pair = np.stack(test_img_pair)\n",
    "test_label_pair = np.stack(test_label_pair)\n",
    "test_label_pair = (test_label_pair[:,0] == test_label_pair[:,1])\n",
    "test_label_pair = test_label_pair*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for training, we have a pair of feature vectors obtained from LBPH per label. However, a classifier needs only one feature vector per sample. We can convert two feature vectors into one by concatenation. However, here we just subtract the two feature vectors two form a single feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pair = np.abs(img_pair[:,0,...] - img_pair[:,1,...])\n",
    "# test_img_pair = np.abs(test_img_pair[:,0,...] - test_img_pair[:,1,...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the training data and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training data\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "g = np.random.permutation(img_pair.shape[0])\n",
    "img_pair = img_pair[g,:]\n",
    "label_pair = label_pair[g,]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(probability=True,gamma='scale')\n",
    "# clf = DecisionTreeClassifier()\n",
    "# clf = LogisticRegression()\n",
    "# clf = GradientBoostingClassifier()\n",
    "clf.fit(img_pair,label_pair)\n",
    "img_pair.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(test_img_pair,test_label_pair))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(test_img_pair)\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(clf.predict(test_img_pair),test_label_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imbalance and bias in the classifier\n",
    "\n",
    "How many positive pairs are present in the training data?\n",
    "\n",
    "How many negative pairs are present in the training data?\n",
    "\n",
    "What's the default accuracy of the classifier?\n",
    "\n",
    "What metric to choose to fairly evaluate the classifier?\n",
    "\n",
    "References:\n",
    "- https://stats.stackexchange.com/questions/90779/area-under-the-roc-curve-or-area-under-the-pr-curve-for-imbalanced-data\n",
    "- https://www.kaggle.com/general/7517#post41179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the precision-recall curve\n",
    "\n",
    "from sklearn.metrics import auc,precision_recall_curve\n",
    "precision,recall,thr = precision_recall_curve(test_label_pair, pred[:,1], pos_label=1)\n",
    "print('Number of thresholds evaluated: ',len(thr))\n",
    "plt.plot(recall,precision)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = (2*precision*recall)/(precision+recall)\n",
    "print('max f1-score:',np.amax(f1_score))\n",
    "pr_auc = auc(recall,precision)\n",
    "print('Area under Precision Recall Curve: ',pr_auc)\n",
    "print (np.argmax(f1_score))\n",
    "# print (thr[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge\n",
    "\n",
    "- #### Robustness to different nuisance parameters: \n",
    "Given a pair of images your trained model should be able to tell whether the faces in the images belong to the same person or not. The test images can be captured under various lighting conditions with various backgrounds. Your trained model should be robust to lighting/illumination changes, image resolution, image aspect ratio, background clutter, morphing of faces (like expressions, wearing glasses etc.) and so on. However, the only thing that can be assured would be that all faces in the test set will be frontal and not side profiles of the faces.\n",
    "- #### Collecting dataset and cleaning the data:\n",
    "A very important aspect in making the trained model robust to all the nuisance factors would be to collect dataset with all such nuisance and train your model on such a dataset. Another important step would be to preprocess the images with standard techniques (like image normalization, http://www.idiap.ch/~marcel/labs/faceverif.php) etc. \n",
    "- #### Feature selection and feature extraction: \n",
    "You are free to extract features of your choice from the given data. The features could be the ones mentioned above or you could find any of your own features. You could use data-dependent features (like PCA or eigenfaces) or data-independent features (like LBPH). You can use only one feature for the classifier or you can use a combination of multiple features as well.\n",
    "- #### Choice of classifier:\n",
    "Note that the choice of classifier matters very little to the final score evaluation. What matters most is how diverse a dataset can you collect and how good are the extracted features. However, setting good parameters for the classifier also matters. Hence, search for the best hyperparameters. The only requirement on your classifier is that it should be able to ouput probability scores so as to compute the performance metrics.\n",
    "- #### Performance Evaluation:\n",
    "The performance will be evaluated based on two metrics : Max F1-score and the area under the precision recall curve for the test set.\n",
    "- We will open a Kaggle competition soon where you have to upload your code and trained model. There will be a public test set on which your code should run and there will be a private test set based on which we will evaluate the final score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- #### List of different face image datasets: \n",
    "http://www.face-rec.org/databases/\n",
    "- #### Some papers on face recognition:\n",
    "[Face recognition with local binary patterns](https://pdfs.semanticscholar.org/3242/0c65f8ef0c5bd83b14c8ae662cbce73e6781.pdf)\n",
    "\n",
    "[Face description with local binary patterns: Application to face recognition](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf)\n",
    "\n",
    "[Deepface: Closing the gap to human-level performance in face verification](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf)\n",
    "\n",
    "[Face recognition using eigenfaces](http://www.cin.ufpe.br/~rps/Artigos/Face%20Recognition%20Using%20Eigenfaces.pdf)\n",
    "\n",
    "[Kernel eigenfaces vs. kernel fisherfaces: Face recognition using kernel methods](http://faculty.ucmerced.edu/mhyang/papers/fg02.pdf)\n",
    "\n",
    "\n",
    "#### Image courtesy\n",
    "- #### LBP images taken from Towards Data Science blog:\n",
    "https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b\n",
    "- #### Face detection code courtesy:\n",
    "https://github.com/shantnu/FaceDetect/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(data):\n",
    "    hists = []\n",
    "    x_grid = 7\n",
    "    y_grid = 7\n",
    "    for ind,img in enumerate(data):\n",
    "        img = local_binary_pattern(img,3,3,method='nri_uniform')\n",
    "        grid_size_x = img.shape[0]//x_grid\n",
    "        grid_size_y = img.shape[1]//y_grid\n",
    "        for x in range(x_grid):\n",
    "            for y in range(y_grid):\n",
    "                patch = img[x*grid_size_x:(x+1)*grid_size_x,y*grid_size_y:(y+1)*grid_size_y]\n",
    "                hist,_ = np.histogram(patch.reshape(-1),bins=64)\n",
    "                if x==0 and y==0:\n",
    "                    patch_hist_list = hist\n",
    "                else:\n",
    "                    patch_hist_list = np.concatenate([patch_hist_list,hist],0)\n",
    "        hists.append(patch_hist_list.reshape(1,-1))\n",
    "\n",
    "    return np.concatenate(hists,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skimage import io, exposure\n",
    "\n",
    "df = pd.read_csv('image_pairs.csv')\n",
    "\n",
    "i = 0\n",
    "hf = h5py.File('test.h5', 'r')\n",
    "keys = list(hf.keys())\n",
    "data = map(hf.get,keys)\n",
    "\n",
    "# hist_vector = create_vector(data)\n",
    "hog_features = [hog(crop_face(n),pixels_per_cell=(5,5)) for n in data]\n",
    "hog_features = np.stack(hog_features,0)\n",
    "hog_features = hog_features.reshape(len(data),-1)\n",
    "# pca.fit(hist_vector)\n",
    "# n_components = np.argmin(np.cumsum(pca.explained_variance_ratio_)<=0.95)\n",
    "dc = dict(zip(keys,pca.transform(hist_vector)[:,:n_components]))\n",
    "# print (plt.bar(range(79),dc[155]),plt.bar(range(79),dc[140]))\n",
    "# print (dc.keys())\n",
    "# hf.close()\n",
    "print(n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testpred = []\n",
    "\n",
    "for ind,row in df.iterrows():\n",
    "    \n",
    "#     testpred.append(cosine_similarity(dc[str(row['pair1'])].reshape(1,-1), dc[str(row['pair2'])].reshape(1,-1)))\n",
    "    testpred.append(np.abs(dc[str(row['pair1'])] - dc[str(row['pair2'])]))\n",
    "probs = clf.predict_proba(testpred)\n",
    "testpred = (probs[:,1]>0.97).astype(int)\n",
    "# testpred = (np.array(testpred).squeeze() > 0.8).astype(int)\n",
    "pd.DataFrame([range(df.shape[0]),testpred],index = ['Id','Expected']).T.to_csv('submisssion.csv',index=False)\n",
    "    \n",
    "# plt.figure(1)\n",
    "# plt.imshow(hf['54'][:],cmap='Greys_r')\n",
    "# plt.figure(2)\n",
    "# plt.imshow(hf['144'][:],cmap='Greys_r')\n",
    "# test1 = np.concatenate(pair1)\n",
    "# test2 = np.concatenate(pair2)\n",
    "# test1.shape, test2.shape\n",
    "testpred\n",
    "# print(np.mean(testpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dire = './test/'\n",
    "dic = {}\n",
    "for i in range(1,24):\n",
    "    ls = os.listdir('./test/'+str(i))\n",
    "    for j in ls:\n",
    "        dic[int(j[:-4])] = i\n",
    "test = df.applymap(dic.get)\n",
    "true_label = (test['pair1'] == test['pair2']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_label,testpred)\n",
    "# np.where(true_label ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
